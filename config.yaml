ctransformers:
  model_path:
    small: "./models/mistral-7b-instruct-v0.1.Q3_K_M.gguf"
    large: "D:/QuanAI/Models/uonlp/Vistral-7B-Chat-gguf/vitral-7b-chat.Q5_K_M.gguf"

  model_type: "gpt2"
  model_config:
    'max_new_tokens': 256
    'temperature': 0.2
    'context_length': 2048
    'gpu_layers': 32 # 32 to put all mistral layers on GPU, might differ for other models
    'threads': -1


chat_config:
  chat_memory_length: 2
  number_of_retrieved_documents: 3

pdf_text_splitter:
  chunk_size: 1024 # number of characters 1024 roughly equels 256 tokens
  overlap: 50
  separators: [ "\n", "\n\n" ]

llava_model:
  llava_model_path: "D:/QuanAI/Models/idefics_9b_instruct"
  clip_model_path: "D:/QuanAI/Models/llava-gguf/mmproj-model-f16.gguf"
  checkpoint_path: "D:/QuanAI/Models/idefics_9b_instruct_CBD/QuanAI-CBD-PCBA-Images/full"

whisper_model: "D:/HaoDZ/Hao/models"

embeddings_path: "D:/QuanAI/llm-webui/rag/embedding_models/sentence-transformers/all-MiniLM-L6-v2"

chromadb:
  chromadb_path: "chroma_db"
  collection_name: "pdfs"

chat_sessions_database_path: "./chat_sessions/chat_sessions.db"